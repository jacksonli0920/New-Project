lr= 0.001 epoch = 2 batch_size=4


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


[1,  2000] loss: 2.198
[1,  4000] loss: 1.889
[1,  6000] loss: 1.688
[1,  8000] loss: 1.582
[1, 10000] loss: 1.524
[1, 12000] loss: 1.484
[2,  2000] loss: 1.412
[2,  4000] loss: 1.371
[2,  6000] loss: 1.361
[2,  8000] loss: 1.305
[2, 10000] loss: 1.301
[2, 12000] loss: 1.299


lr= 0.001 epoch = 10 batch_size=4

[1,  2000] loss: 2.220
[1,  4000] loss: 1.942
[1,  6000] loss: 1.688
[1,  8000] loss: 1.565
[1, 10000] loss: 1.503
[1, 12000] loss: 1.476
[2,  2000] loss: 1.397
[2,  4000] loss: 1.363
[2,  6000] loss: 1.331
[2,  8000] loss: 1.304
[2, 10000] loss: 1.284
[2, 12000] loss: 1.257
[3,  2000] loss: 1.183
[3,  4000] loss: 1.181
[3,  6000] loss: 1.208
[3,  8000] loss: 1.152
[3, 10000] loss: 1.166
[3, 12000] loss: 1.155
[4,  2000] loss: 1.094
[4,  4000] loss: 1.084
[4,  6000] loss: 1.092
[4,  8000] loss: 1.091
[4, 10000] loss: 1.056
[4, 12000] loss: 1.085
[5,  2000] loss: 0.994
[5,  4000] loss: 1.030
[5,  6000] loss: 1.017
[5,  8000] loss: 1.027
[5, 10000] loss: 1.031
[5, 12000] loss: 1.025
[6,  2000] loss: 0.927
[6,  4000] loss: 0.961
[6,  6000] loss: 0.966
[6,  8000] loss: 0.981
[6, 10000] loss: 0.985
[6, 12000] loss: 0.972
[7,  2000] loss: 0.885
[7,  4000] loss: 0.910
[7,  6000] loss: 0.935
[7,  8000] loss: 0.927
[7, 10000] loss: 0.940
[7, 12000] loss: 0.925
[8,  2000] loss: 0.863
[8,  4000] loss: 0.885
[8,  6000] loss: 0.884
[8,  8000] loss: 0.897
[8, 10000] loss: 0.910
[8, 12000] loss: 0.896
[9,  2000] loss: 0.785
[9,  4000] loss: 0.820
[9,  6000] loss: 0.875
[9,  8000] loss: 0.882
[9, 10000] loss: 0.870
[9, 12000] loss: 0.880
[10,  2000] loss: 0.766
[10,  4000] loss: 0.796
[10,  6000] loss: 0.808
[10,  8000] loss: 0.816
[10, 10000] loss: 0.847
[10, 12000] loss: 0.856


lr= 0.001 epoch = 10 batch_size=8

[1,  2000] loss: 2.158
[1,  4000] loss: 1.766
[1,  6000] loss: 1.591
[2,  2000] loss: 1.467
[2,  4000] loss: 1.408
[2,  6000] loss: 1.360
[3,  2000] loss: 1.286
[3,  4000] loss: 1.244
[3,  6000] loss: 1.209
[4,  2000] loss: 1.152
[4,  4000] loss: 1.150
[4,  6000] loss: 1.124
[5,  2000] loss: 1.059
[5,  4000] loss: 1.085
[5,  6000] loss: 1.054
[6,  2000] loss: 0.996
[6,  4000] loss: 1.002
[6,  6000] loss: 1.011
[7,  2000] loss: 0.932
[7,  4000] loss: 0.952
[7,  6000] loss: 0.952
[8,  2000] loss: 0.884
[8,  4000] loss: 0.882
[8,  6000] loss: 0.913
[9,  2000] loss: 0.837
[9,  4000] loss: 0.855
[9,  6000] loss: 0.863
[10,  2000] loss: 0.780
[10,  4000] loss: 0.814
[10,  6000] loss: 0.846



Use sigmoid activation function 

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.sigmoid(self.conv1(x)))
        x = self.pool(F.sigmoid(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.sigmoid(self.fc1(x))
        x = F.sigmoid(self.fc2(x))
        x = self.fc3(x)
        return x

converge slower than relu

[1,  2000] loss: 2.317
[1,  4000] loss: 2.313
[1,  6000] loss: 2.313
[1,  8000] loss: 2.313
[1, 10000] loss: 2.312
[1, 12000] loss: 2.311
[2,  2000] loss: 2.311
[2,  4000] loss: 2.309
[2,  6000] loss: 2.308
[2,  8000] loss: 2.309
[2, 10000] loss: 2.307
[2, 12000] loss: 2.308
[3,  2000] loss: 2.306
[3,  4000] loss: 2.307
[3,  6000] loss: 2.307
[3,  8000] loss: 2.305
[3, 10000] loss: 2.306
[3, 12000] loss: 2.306
[4,  2000] loss: 2.305
[4,  4000] loss: 2.305
[4,  6000] loss: 2.304
[4,  8000] loss: 2.305
[4, 10000] loss: 2.304
[4, 12000] loss: 2.304
[5,  2000] loss: 2.304
[5,  4000] loss: 2.305
[5,  6000] loss: 2.304
[5,  8000] loss: 2.304
[5, 10000] loss: 2.304
[5, 12000] loss: 2.304
[6,  2000] loss: 2.303
[6,  4000] loss: 2.304
[6,  6000] loss: 2.303
[6,  8000] loss: 2.303
[6, 10000] loss: 2.303
[6, 12000] loss: 2.303
[7,  2000] loss: 2.302
[7,  4000] loss: 2.302
[7,  6000] loss: 2.301
[7,  8000] loss: 2.300
[7, 10000] loss: 2.299
[7, 12000] loss: 2.294
[8,  2000] loss: 2.286
[8,  4000] loss: 2.271
[8,  6000] loss: 2.218
[8,  8000] loss: 2.138
[8, 10000] loss: 2.081
[8, 12000] loss: 2.067
[9,  2000] loss: 2.067
[9,  4000] loss: 2.061
[9,  6000] loss: 2.050
[9,  8000] loss: 2.054
[9, 10000] loss: 2.055
[9, 12000] loss: 2.048
[10,  2000] loss: 2.032
[10,  4000] loss: 2.021
[10,  6000] loss: 2.028
[10,  8000] loss: 2.014
[10, 10000] loss: 1.992
[10, 12000] loss: 1.970


final results:

relu activation, batch size 4 epoch 10


[1,  2000] loss: 2.212
[1,  4000] loss: 1.848
[1,  6000] loss: 1.673
[1,  8000] loss: 1.581
[1, 10000] loss: 1.506
[1, 12000] loss: 1.480
[2,  2000] loss: 1.365
[2,  4000] loss: 1.379
[2,  6000] loss: 1.317
[2,  8000] loss: 1.330
[2, 10000] loss: 1.291
[2, 12000] loss: 1.279
[3,  2000] loss: 1.198
[3,  4000] loss: 1.206
[3,  6000] loss: 1.191
[3,  8000] loss: 1.212
[3, 10000] loss: 1.167
[3, 12000] loss: 1.150
[4,  2000] loss: 1.089
[4,  4000] loss: 1.116
[4,  6000] loss: 1.088
[4,  8000] loss: 1.103
[4, 10000] loss: 1.090
[4, 12000] loss: 1.122
[5,  2000] loss: 1.017
[5,  4000] loss: 1.022
[5,  6000] loss: 1.060
[5,  8000] loss: 1.052
[5, 10000] loss: 1.043
[5, 12000] loss: 1.038
[6,  2000] loss: 0.946
[6,  4000] loss: 0.991
[6,  6000] loss: 0.983
[6,  8000] loss: 0.971
[6, 10000] loss: 0.980
[6, 12000] loss: 0.996
[7,  2000] loss: 0.908
[7,  4000] loss: 0.915
[7,  6000] loss: 0.912
[7,  8000] loss: 0.952
[7, 10000] loss: 0.960
[7, 12000] loss: 0.954
[8,  2000] loss: 0.860
[8,  4000] loss: 0.886
[8,  6000] loss: 0.895
[8,  8000] loss: 0.893
[8, 10000] loss: 0.906
[8, 12000] loss: 0.918
[9,  2000] loss: 0.789
[9,  4000] loss: 0.842
[9,  6000] loss: 0.881
[9,  8000] loss: 0.883
[9, 10000] loss: 0.867
[9, 12000] loss: 0.902
[10,  2000] loss: 0.773
[10,  4000] loss: 0.797
[10,  6000] loss: 0.839
[10,  8000] loss: 0.840
[10, 10000] loss: 0.855
[10, 12000] loss: 0.866

Accuracy of the network on the 10000 test images: 62 %

